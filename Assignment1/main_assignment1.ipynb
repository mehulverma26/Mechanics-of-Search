{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import math\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Dataset and Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Cranfield Dataset\n",
    "def load_cranfield_dataset(filename):\n",
    "    with open(filename, 'r', encoding='utf-8-sig') as file:\n",
    "        content = file.read().strip()\n",
    "    if not content.startswith('<?xml'):\n",
    "        content = '<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<root>\\n' + content + '\\n</root>'\n",
    "    root = ET.fromstring(content)\n",
    "    documents = {}\n",
    "    for doc in root.findall('doc'):\n",
    "        doc_id_elem = doc.find('docno')\n",
    "        text_elem = doc.find('text')\n",
    "        if doc_id_elem is None:\n",
    "            continue\n",
    "        doc_id = doc_id_elem.text.strip()\n",
    "        text = text_elem.text.strip() if text_elem is not None and text_elem.text else ''\n",
    "        documents[doc_id] = preprocess(text)\n",
    "    return documents\n",
    "\n",
    "# Load Queries\n",
    "def load_queries(filename):\n",
    "    with open(filename, 'r', encoding='utf-8-sig') as file:\n",
    "        content = file.read().strip()\n",
    "    if not content.startswith('<?xml'):\n",
    "        content = '<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<root>\\n' + content + '\\n</root>'\n",
    "    root = ET.fromstring(content)\n",
    "    queries = {}\n",
    "    for i, query in enumerate(root.findall('top'), start=1):\n",
    "        text_elem = query.find('title')\n",
    "        text = text_elem.text.strip() if text_elem is not None and text_elem.text else ''\n",
    "        queries[str(i)] = preprocess(text)\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Interted Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Function\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    tokens = text.split()\n",
    "    return [token for token in tokens if token.isalnum()]\n",
    "\n",
    "# Build Inverted Index\n",
    "def build_inverted_index(documents):\n",
    "    inverted_index = defaultdict(dict)\n",
    "    doc_lengths = defaultdict(int)\n",
    "    for doc_id, text in documents.items():\n",
    "        term_freqs = defaultdict(int)\n",
    "        for term in text:\n",
    "            term_freqs[term] += 1\n",
    "        for term, freq in term_freqs.items():\n",
    "            inverted_index[term][doc_id] = freq\n",
    "        doc_lengths[doc_id] = len(text)\n",
    "    return inverted_index, doc_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Space Model (TF-IDF)\n",
    "def compute_tfidf_scores(query, inverted_index, doc_lengths, total_docs):\n",
    "    query_terms = query\n",
    "    scores = defaultdict(float)\n",
    "    for term in query_terms:\n",
    "        if term in inverted_index:\n",
    "            doc_freq = len(inverted_index[term])\n",
    "            idf = math.log((total_docs / (1 + doc_freq)))\n",
    "            for doc_id, term_freq in inverted_index[term].items():\n",
    "                tf = term_freq / doc_lengths[doc_id]\n",
    "                scores[doc_id] += tf * idf\n",
    "    return sorted(scores.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 Model\n",
    "def compute_bm25_scores(query, inverted_index, doc_lengths, total_docs, k1=1.5, b=0.75):\n",
    "    avg_doc_len = sum(doc_lengths.values()) / total_docs\n",
    "    query_terms = query\n",
    "    scores = defaultdict(float)\n",
    "    for term in query_terms:\n",
    "        if term in inverted_index:\n",
    "            doc_freq = len(inverted_index[term])\n",
    "            idf = math.log((total_docs - doc_freq + 0.5) / (doc_freq + 0.5) + 1)\n",
    "            for doc_id, term_freq in inverted_index[term].items():\n",
    "                tf = term_freq\n",
    "                doc_len = doc_lengths[doc_id]\n",
    "                score = idf * ((tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_len / avg_doc_len))))\n",
    "                scores[doc_id] += score\n",
    "    return sorted(scores.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LM with smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language Model with Smoothing\n",
    "def compute_lm_scores(query, documents, lambda_=0.1):\n",
    "    query_terms = query\n",
    "    doc_scores = {}\n",
    "    collection_freqs = defaultdict(int)\n",
    "    total_terms = sum(len(doc) for doc in documents.values())\n",
    "    for doc_text in documents.values():\n",
    "        for token in doc_text:\n",
    "            collection_freqs[token] += 1\n",
    "    for doc_id, text in documents.items():\n",
    "        doc_len = len(text)\n",
    "        term_freqs = defaultdict(int)\n",
    "        for token in text:\n",
    "            term_freqs[token] += 1\n",
    "        score = 0\n",
    "        for token in query_terms:\n",
    "            p_td = (term_freqs[token] + 1) / (doc_len + len(collection_freqs))\n",
    "            p_tc = collection_freqs[token] / total_terms if collection_freqs[token] > 0 else 1e-10\n",
    "            score += math.log((lambda_ * p_td) + ((1 - lambda_) * p_tc))\n",
    "        doc_scores[doc_id] = score\n",
    "    return sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Results\n",
    "def write_results(output_file, model_name, results):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for query_id, ranked_docs in results.items():\n",
    "            for rank, (doc_id, score) in enumerate(ranked_docs[:100], start=1):\n",
    "                f.write(f'{query_id} 0 {doc_id} {rank} {score} {model_name}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval and ranking complete. Use trec_eval for evaluation.\n"
     ]
    }
   ],
   "source": [
    "# Main Execution\n",
    "documents = load_cranfield_dataset('cran.all.1400.xml')\n",
    "queries = load_queries('cran.qry.xml')\n",
    "\n",
    "inverted_index, doc_lengths = build_inverted_index(documents)\n",
    "total_docs = len(documents)\n",
    "\n",
    "tfidf_results = {q_id: compute_tfidf_scores(q, inverted_index, doc_lengths, total_docs) for q_id, q in queries.items()}\n",
    "bm25_results = {q_id: compute_bm25_scores(q, inverted_index, doc_lengths, total_docs) for q_id, q in queries.items()}\n",
    "lm_results = {q_id: compute_lm_scores(q, documents) for q_id, q in queries.items()}\n",
    "\n",
    "write_results('tfidf_results.txt', 'TFIDF', tfidf_results)\n",
    "write_results('bm25_results.txt', 'BM25', bm25_results)\n",
    "write_results('lm_results.txt', 'LM', lm_results)\n",
    "\n",
    "print('Retrieval and ranking complete. Use trec_eval for evaluation.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
