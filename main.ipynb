{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Cranfield dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cranfield_dataset(filename):\n",
    "    \"\"\"Loads documents from the Cranfield dataset, ensuring correct XML parsing.\"\"\"\n",
    "    with open(filename, \"r\", encoding=\"utf-8-sig\") as file:\n",
    "        content = file.read().strip()\n",
    "\n",
    "    # Ensure XML has a single root element\n",
    "    if not content.startswith(\"<?xml\"):\n",
    "        content = \"<?xml version='1.0' encoding='utf-8'?>\\n<root>\\n\" + content + \"\\n</root>\"\n",
    "\n",
    "    root = ET.fromstring(content)\n",
    "    documents = {}\n",
    "\n",
    "    for doc in root.findall(\"doc\"):\n",
    "        doc_id_elem = doc.find(\"docno\")\n",
    "        text_elem = doc.find(\"text\")\n",
    "\n",
    "        if doc_id_elem is None:\n",
    "            print(\"Warning: Skipping document with missing <docno>.\")\n",
    "            continue\n",
    "\n",
    "        doc_id = doc_id_elem.text.strip()\n",
    "        text = text_elem.text.strip() if text_elem is not None and text_elem.text else \"\"  # Handle missing <text>\n",
    "\n",
    "        documents[doc_id] = text\n",
    "\n",
    "    return documents\n",
    "\n",
    "def load_queries(filename):\n",
    "    \"\"\"Loads queries from the Cranfield dataset, ensuring correct XML parsing.\"\"\"\n",
    "    with open(filename, \"r\", encoding=\"utf-8-sig\") as file:\n",
    "        content = file.read().strip()\n",
    "\n",
    "    if not content.startswith(\"<?xml\"):\n",
    "        content = \"<?xml version='1.0' encoding='utf-8'?>\\n<root>\\n\" + content + \"\\n</root>\"\n",
    "\n",
    "    root = ET.fromstring(content)\n",
    "    queries = {}\n",
    "\n",
    "    for query in root.findall(\"top\"):\n",
    "        query_id_elem = query.find(\"num\")\n",
    "        text_elem = query.find(\"title\")\n",
    "\n",
    "        if query_id_elem is None:\n",
    "            print(\"Warning: Skipping query with missing <num>.\")\n",
    "            continue\n",
    "\n",
    "        query_id = query_id_elem.text.strip()\n",
    "        text = text_elem.text.strip() if text_elem is not None and text_elem.text else \"\"  # Handle missing <title>\n",
    "\n",
    "        queries[query_id] = text\n",
    "\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tfidf_index(documents):\n",
    "    \"\"\"Builds a TF-IDF index using sklearn's TfidfVectorizer.\"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    doc_ids = list(documents.keys())\n",
    "    doc_texts = list(documents.values())\n",
    "    tfidf_matrix = vectorizer.fit_transform(doc_texts)\n",
    "    return vectorizer, tfidf_matrix, doc_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM25 Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bm25_index(documents):\n",
    "    \"\"\"Builds a BM25 index using the rank_bm25 library.\"\"\"\n",
    "    tokenized_corpus = [doc.split() for doc in documents.values()]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    return bm25, list(documents.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model (Jelinek-Mercer Smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lm_scores(query, documents, lambda_=0.1):\n",
    "    \"\"\"Computes scores using a Language Model with Dirichlet smoothing.\"\"\"\n",
    "    query_tokens = query.split()\n",
    "    doc_scores = {}\n",
    "    collection_freqs = defaultdict(int)\n",
    "    total_terms = sum(len(doc.split()) for doc in documents.values())\n",
    "\n",
    "    # Compute collection frequency\n",
    "    for doc_text in documents.values():\n",
    "        for token in doc_text.split():\n",
    "            collection_freqs[token] += 1\n",
    "    \n",
    "    for doc_id, text in documents.items():\n",
    "        doc_tokens = text.split()\n",
    "        doc_len = len(doc_tokens)\n",
    "        term_freqs = defaultdict(int)\n",
    "        \n",
    "        for token in doc_tokens:\n",
    "            term_freqs[token] += 1\n",
    "        \n",
    "        score = 0\n",
    "        for token in query_tokens:\n",
    "            p_td = (term_freqs[token] + 1) / (doc_len + len(collection_freqs))  # Add-one smoothing\n",
    "            p_tc = collection_freqs[token] / total_terms if collection_freqs[token] > 0 else 1e-10  # Avoid zero division\n",
    "            score += math.log((lambda_ * p_td) + ((1 - lambda_) * p_tc))\n",
    "        \n",
    "        doc_scores[doc_id] = score\n",
    "    \n",
    "    return doc_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tfidf(query, vectorizer, tfidf_matrix, doc_ids):\n",
    "    \"\"\"Searches using TF-IDF and ranks documents.\"\"\"\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    scores = np.dot(tfidf_matrix, query_vector.T).toarray().flatten()\n",
    "    ranked_docs = sorted(zip(doc_ids, scores), key=lambda x: x[1], reverse=True)\n",
    "    return ranked_docs\n",
    "\n",
    "def search_bm25(query, bm25, doc_ids):\n",
    "    \"\"\"Searches using BM25 ranking function.\"\"\"\n",
    "    scores = bm25.get_scores(query.split())\n",
    "    ranked_docs = sorted(zip(doc_ids, scores), key=lambda x: x[1], reverse=True)\n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Output File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results(output_file, model_name, results):\n",
    "    \"\"\"Writes ranking results to a file in trec_eval format.\"\"\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for query_id, ranked_docs in results.items():\n",
    "            for rank, (doc_id, score) in enumerate(ranked_docs[:100], start=1):  # Top 100 results\n",
    "                f.write(f\"{query_id} 0 {doc_id} {rank} {score} {model_name}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval and ranking complete. Use trec_eval for evaluation.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset and queries\n",
    "documents = load_cranfield_dataset(\"cran.all.1400.xml\")\n",
    "queries = load_queries(\"cran.qry.xml\")\n",
    "\n",
    "# Build indexes\n",
    "vectorizer, tfidf_matrix, doc_ids = build_tfidf_index(documents)\n",
    "bm25, _ = build_bm25_index(documents)\n",
    "\n",
    "# Perform retrieval\n",
    "tfidf_results = {q_id: search_tfidf(q, vectorizer, tfidf_matrix, doc_ids) for q_id, q in queries.items()}\n",
    "bm25_results = {q_id: search_bm25(q, bm25, doc_ids) for q_id, q in queries.items()}\n",
    "lm_results = {q_id: sorted(compute_lm_scores(q, documents).items(), key=lambda x: x[1], reverse=True) for q_id, q in queries.items()}\n",
    "\n",
    "# Write results\n",
    "write_results(\"tfidf_results.txt\", \"TFIDF\", tfidf_results)\n",
    "write_results(\"bm25_results.txt\", \"BM25\", bm25_results)\n",
    "write_results(\"lm_results.txt\", \"LM\", lm_results)\n",
    "\n",
    "print(\"Retrieval and ranking complete. Use trec_eval for evaluation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
