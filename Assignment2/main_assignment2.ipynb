{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import math\n",
    "import shutil\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create folders for images and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"images\", exist_ok=True)\n",
    "os.makedirs(\"metadata\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crawler to Collect Images and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_images(base_url, max_images=1000):\n",
    "    visited = set()\n",
    "    image_data = {}\n",
    "    queue = [base_url]\n",
    "\n",
    "    while queue and len(image_data) < max_images:\n",
    "        url = queue.pop(0)\n",
    "        if url in visited:\n",
    "            continue\n",
    "        visited.add(url)\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            if response.status_code != 200:\n",
    "                continue\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            for img in soup.find_all(\"img\"):\n",
    "                img_url = urljoin(url, img.get(\"src\", \"\"))\n",
    "                alt_text = img.get(\"alt\", \"No caption\").strip()\n",
    "\n",
    "                if img_url and len(image_data) < max_images:\n",
    "                    img_name = f\"image_{len(image_data)}.jpg\"\n",
    "                    img_path = os.path.join(\"images\", img_name)\n",
    "\n",
    "                    try:\n",
    "                        img_data = requests.get(img_url, stream=True, timeout=5)\n",
    "                        with open(img_path, \"wb\") as f:\n",
    "                            shutil.copyfileobj(img_data.raw, f)\n",
    "\n",
    "                        image_data[img_name] = alt_text\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "            for link in soup.find_all(\"a\", href=True):\n",
    "                new_url = urljoin(url, link[\"href\"])\n",
    "                if new_url.startswith(base_url) and new_url not in visited:\n",
    "                    queue.append(new_url)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return image_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    tokens = text.split()\n",
    "    return [token for token in tokens if token.isalnum()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inverted_index(metadata):\n",
    "    inverted_index = defaultdict(dict)\n",
    "    doc_lengths = {}\n",
    "\n",
    "    for img_name, text in metadata.items():\n",
    "        tokens = preprocess(text)\n",
    "        term_freqs = defaultdict(int)\n",
    "\n",
    "        for term in tokens:\n",
    "            term_freqs[term] += 1\n",
    "\n",
    "        for term, freq in term_freqs.items():\n",
    "            inverted_index[term][img_name] = freq\n",
    "\n",
    "        doc_lengths[img_name] = len(tokens)\n",
    "\n",
    "    return inverted_index, doc_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf_scores(query, inverted_index, doc_lengths, total_docs):\n",
    "    query_terms = preprocess(query)\n",
    "    scores = defaultdict(float)\n",
    "\n",
    "    for term in query_terms:\n",
    "        if term in inverted_index:\n",
    "            doc_freq = len(inverted_index[term])\n",
    "            idf = math.log((total_docs / (1 + doc_freq)))\n",
    "            for img_name, term_freq in inverted_index[term].items():\n",
    "                tf = term_freq / doc_lengths[img_name]\n",
    "                scores[img_name] += tf * idf\n",
    "\n",
    "    return sorted(scores.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    base_url = \"https://en.wikipedia.org/wiki/Artificial_intelligence\"\n",
    "    image_metadata = crawl_images(base_url, max_images=1000)\n",
    "\n",
    "    inverted_index, doc_lengths = build_inverted_index(image_metadata)\n",
    "    total_docs = len(image_metadata)\n",
    "\n",
    "    # Example query\n",
    "    query = \"robotics\"\n",
    "    results = compute_tfidf_scores(query, inverted_index, doc_lengths, total_docs)\n",
    "\n",
    "    print(\"Search Results for query:\", query)\n",
    "    for img, score in results[:10]:\n",
    "        print(f\"{img}: {score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
